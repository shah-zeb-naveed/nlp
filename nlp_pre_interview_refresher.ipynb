{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/nlp/blob/main/nlp_pre_interview_refresher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP - Zero to Hero"
      ],
      "metadata": {
        "id": "rpD9s7kANeGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6EBuxDf8vfF5"
      },
      "outputs": [],
      "source": [
        "# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
        "# https://pberba.github.io/stats/2020/07/08/intro-hdbscan/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "36f-Q0WtvfCh"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pyLDAvis scikit-learn hdbscan scipy==1.10.1 svgling --quiet --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import warning and disable\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "2h9dwXFysc9d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kNfQ119kve_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0f9db6-4338-4326-8ba3-db97dd4a4e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/hdbscan/plots.py:448: DeprecationWarning: invalid escape sequence '\\l'\n",
            "  axis.set_ylabel('$\\lambda$ value')\n",
            "/usr/local/lib/python3.11/dist-packages/hdbscan/robust_single_linkage_.py:154: DeprecationWarning: invalid escape sequence '\\{'\n",
            "  \"\"\"Perform robust single linkage clustering from a vector array\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "assert scipy.__version__ == '1.10.1'\n",
        "from __future__ import print_function\n",
        "import pyLDAvis\n",
        "#import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "#import torch\n",
        "#from sentence_transformers import SentenceTransformer\n",
        "#from bertopic import BERTopic\n",
        "#from umap import UMAP\n",
        "\n",
        "import hdbscan\n",
        "import logging\n",
        "logging.basicConfig()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "bnuHnjAJPvhn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define docs"
      ],
      "metadata": {
        "id": "-wbJfySTl6fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnSvfrKkv-ub",
        "outputId": "39b96d3e-c37f-456a-d849-77bf14d4d978"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data'][0:1000]\n",
        "\n",
        "docs = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
        "    \"The tower is 324 meters tall, about the same height as an 81-story building.\",\n",
        "    \"It held this title for 41 years until the Chrysler Building in New York City was finished in 1930.\",\n",
        "    \"The tower has three levels for visitors, with restaurants on the first and second levels.\",\n",
        "    \"Tickets can be purchased to ascend by stairs or lift to the first and second levels.\",\n",
        "]\n",
        "\n",
        "\n",
        "# # Save the documents to disk\n",
        "# with open('docs.pkl', 'wb') as file:\n",
        "#     pickle.dump(docs, file)\n",
        "\n",
        "docs[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('docs.pkl', 'rb') as file:\n",
        "#     docs = pickle.load(file)"
      ],
      "metadata": {
        "id": "kFK-Bcl7KGZM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "jWlLeyIPl9Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "QpasIcBvmBoD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Qu6VCWHKmBlt",
        "outputId": "6a346bc6-ba82-4ba2-b4c0-a2af08fb6e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>CountVectorizer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>CountVectorizer()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG8pMGFsmBjX",
        "outputId": "418e525c-4a50-4bf8-cc16-a563b3039f56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 19 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze = vectorizer.build_analyzer()\n",
        "analyze(\"This is a text document to analyze.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb268R6omBhA",
        "outputId": "901cf540-c964-4d95-de36-7a86982dc39f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'text', 'document', 'to', 'analyze']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbEf9xTImBe8",
        "outputId": "cc03ee78-39f3-474c-89ad-02bff322ba80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
              "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
              "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.transform(['Something completely new.']).toarray() # ignored"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTGAYAHRmj5x",
        "outputId": "18145145-979c-499a-dcee-3d6027c23019"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to preserve ordering to some extent can use bigrams\n",
        "bigram_counts = CountVectorizer(ngram_range=(1,2))\n",
        "bigram_counts.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ1JQhOAmj3b",
        "outputId": "20eed137-105f-47ea-822b-23328956ada5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_counts.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4vd6ptdmj1F",
        "outputId": "98619a25-491c-4574-c80a-1a99231e68ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'and the', 'document', 'first', 'first document', 'is',\n",
              "       'is the', 'is this', 'one', 'second', 'second document',\n",
              "       'second second', 'the', 'the first', 'the second', 'the third',\n",
              "       'third', 'third one', 'this', 'this is', 'this the'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "J1IboAHSmjyN",
        "outputId": "6db2c284-0c1f-466c-8df2-c7b860b24bbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(smooth_idf=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer(smooth_idf=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>TfidfTransformer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>TfidfTransformer(smooth_idf=False)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = [[3, 0, 1],\n",
        "          [2, 0, 0],\n",
        "          [3, 0, 0],\n",
        "          [4, 0, 0],\n",
        "          [3, 2, 0],\n",
        "          [3, 0, 2]]\n",
        "\n",
        "tfidf = transformer.fit_transform(counts)\n",
        "tfidf\n",
        "\n",
        "tfidf.toarray() # each row a unit vector normalzied by eculidean norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpBH68Wbmjvn",
        "outputId": "1acae03d-431a-4851-e46b-e7b2163e6d7d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 9 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81940995, 0.        , 0.57320793],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [0.47330339, 0.88089948, 0.        ],\n",
              "       [0.58149261, 0.        , 0.81355169]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TfidfTransformer()\n",
        "transformer.fit_transform(counts).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILrkmPBjmjsw",
        "outputId": "ef9e76e1-cc15-4b9e-f4d5-4064e2bdde21"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.85151335, 0.        , 0.52433293],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [1.        , 0.        , 0.        ],\n",
              "       [0.55422893, 0.83236428, 0.        ],\n",
              "       [0.63035731, 0.        , 0.77630514]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.idf_ # weifhts of each feature # maybe related to IDFs (some partial calculation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDQQ_KrxmjqK",
        "outputId": "9f1131df-2386-47fc-85a8-37690522614a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 2.25276297, 1.84729786])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPuAvFHcmjlM",
        "outputId": "85eabc17-eac7-450d-fe5b-bc97c3ad1ae2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "        0.        , 0.35872874, 0.        , 0.43877674],\n",
              "       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
              "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
              "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
              "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
              "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
              "        0.        , 0.35872874, 0.        , 0.43877674]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# short texts noisty tfidf. binary occurrence as used in naive bayes can be set in CountVectorizer (binary=True)"
      ],
      "metadata": {
        "id": "VuU5a8Y8mjit"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bow: cannot capture phrases/multi-words although can use ngrams, order, grammaer, potential misspellings, word derivations\n",
        "# chars can help in misspellings\n",
        "\n",
        "# analyzer is essetnial tokenizer\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
        "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
        "ngram_vectorizer.get_feature_names_out()\n",
        "counts.toarray().astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5oGexxypy_R",
        "outputId": "4f84b897-4d56-4f63-e02e-dd0a6e05c0e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
              "       [1, 1, 0, 1, 1, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
        "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
        "ngram_vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "# across words. more noisy than white-space aware char_wb\n",
        "ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
        "ngram_vectorizer.fit_transform(['jumpy fox'])\n",
        "ngram_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etqW2mfzmBaQ",
        "outputId": "dc9608af-fd85-4270-dd43-e31e95c37295"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x8 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 8 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' fo', ' ju', 'fox', 'jum', 'mpy', 'ox ', 'py ', 'ump'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x7 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 7 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' fo', 'fox', 'jum', 'mpy', 'py ', 'ump', 'y f'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGjXSy2frFCb",
        "outputId": "1505aaad-29f3-4762-b843-dce300c81b70"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'jum': 2, 'ump': 5, 'mpy': 3, 'py ': 4, 'y f': 6, ' fo': 0, 'fox': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2y4dkjqoq26",
        "outputId": "4e31f174-b48a-4f27-a6eb-85db93d20f72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' fo', 'fox', 'jum', 'mpy', 'py ', 'ump', 'y f'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer.get_stop_words()"
      ],
      "metadata": {
        "id": "rBbZsBKZsHDf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2go07Ffnro0p"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Hot Encoding"
      ],
      "metadata": {
        "id": "x2jimh2bUA_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"hello world\", \"hello\", \"world hello\"]\n",
        "\n",
        "# Step 1: Build the vocabulary\n",
        "# Create a set of unique words\n",
        "vocab = set()\n",
        "for sentence in corpus:\n",
        "    vocab.update(sentence.split())\n",
        "\n",
        "# Create a word-to-index dictionary\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Step 2: Create one-hot representations\n",
        "def one_hot_encode(word, word_to_index):\n",
        "    \"\"\"\n",
        "    Create a one-hot vector for the given word.\n",
        "    \"\"\"\n",
        "    vector = torch.zeros(len(word_to_index), dtype=torch.float32)\n",
        "    vector[word_to_index[word]] = 1.0\n",
        "    return vector\n",
        "\n",
        "# Encode the entire corpus\n",
        "one_hot_encoded_corpus = []\n",
        "for sentence in corpus:\n",
        "    encoded_sentence = []\n",
        "    for word in sentence.split():\n",
        "        encoded_sentence.append(one_hot_encode(word, word_to_index))\n",
        "    one_hot_encoded_corpus.append(encoded_sentence)\n",
        "\n",
        "one_hot_encoded_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNhmwg59Touf",
        "outputId": "d6bba28e-cc9d-46af-dfcf-82ff57294d8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[tensor([0., 1.]), tensor([1., 0.])],\n",
              " [tensor([0., 1.])],\n",
              " [tensor([1., 0.]), tensor([0., 1.])]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ord('A')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-13KdGhmWxnR",
        "outputId": "25409f83-5931-4c2b-f0c3-5a41f7d32139"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  This technique is useful for high-cardinality categorical features (e.g., URLs, words, or user IDs) where one-hot encoding is too memory-intensive.\n",
        "\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "import pandas as pd\n",
        "\n",
        "# Sample email subjects\n",
        "emails = [\n",
        "    {\"free\": 1, \"money\": 1, \"win\": 1},     # spam\n",
        "    {\"meeting\": 1, \"schedule\": 1},         # ham\n",
        "    {\"lottery\": 1, \"winner\": 1},           # spam\n",
        "    {\"project\": 1, \"update\": 1}            # ham\n",
        "]\n",
        "\n",
        "# Use FeatureHasher (e.g., 8 hashed features)\n",
        "hasher = FeatureHasher(n_features=8, input_type='dict')\n",
        "X_hashed = hasher.transform(emails)\n",
        "\n",
        "# Convert to array or DataFrame for viewing\n",
        "df_hashed = pd.DataFrame(X_hashed.toarray())\n",
        "print(df_hashed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9SvdhUArqnp",
        "outputId": "ed94fb75-82d7-4208-8616-8307a96e99aa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0    1    2    3    4    5    6    7\n",
            "0  0.0  0.0 -1.0  0.0  0.0  0.0  0.0  0.0\n",
            "1  0.0  0.0  0.0  1.0  0.0 -1.0  0.0  0.0\n",
            "2  0.0  0.0 -1.0  0.0  0.0  0.0 -1.0  0.0\n",
            "3 -1.0 -1.0  0.0  0.0  0.0  0.0  0.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can implement custom hasher or use python's built-in\n",
        "hash('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPP4y6Cnr6sR",
        "outputId": "825242ad-f713-4e61-da0b-88bc84a51d95"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3910120702538823191"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "aSEQn2lKe_Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec (Google) - 2 techniques: Continuous Bag of Words (CBoW) and Skip-Gram;\n",
        "Bow: use context of window size m to predict target. softmax layer and negative log-likelihood given context to train. faster.\n",
        "\n",
        "Skip-gram: use center to predict context in window size m to predict context words. one output layer + softmax predicts (center, context) pair. Avg. neg. log likelihood to train. better for infrequent words.\n",
        "\n",
        "Goal is to learn weights that are the vectors. Both Can use negative sampling to optimize (better for frequent). Hierarchical softmax (better for infrequent words).\n",
        "Linear substructures like vector(man) - vector(woman) + vector(king) = vector(queen). Can average embeddings of multiple words to find closest vectors during inference.\n",
        "\n",
        "\n",
        "Global Vectors or GloVe (Stanford); trained on co-occurence/frequency matrix of word pairs. Can use KNN to get similar words. Linear substructurs: vector difference of man-woman is similar to king-queen. This is encoded as its trained so its dot product equals log of prob ratio. This encodes meaning of the word e.g. ice/steam/gas/water example.\n",
        "\n",
        "fastText (Facebook) —interesting fact: accounts for out of vocabulary words.\n"
      ],
      "metadata": {
        "id": "VMZk8nJVhA7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"I love natural language processing\",\n",
        "    \"word2vec is a cool technique\"\n",
        "]\n",
        "\n",
        "# Tokenize sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model using CBOW\n",
        "model_cbow = Word2Vec(tokenized_sentences, vector_size=100, window=2, min_count=1, sg=0) # switch skip-gram\n",
        "\n",
        "# Get the word vector for a word\n",
        "vector = model_cbow.wv['fox']\n",
        "print(vector)\n",
        "\n",
        "# Find most similar words\n",
        "similar_words = model_cbow.wv.most_similar('fox')\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "id": "mJv9GlWJfAwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # use pre-trained\n",
        "\n",
        "# import gensim.downloader as api\n",
        "\n",
        "# # Load the pre-trained Word2Vec model (Google's pretrained model)\n",
        "# model = api.load(\"word2vec-google-news-300\")\n",
        "# model.most_similar('fox')"
      ],
      "metadata": {
        "id": "stp69JgJfAuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec"
      ],
      "metadata": {
        "id": "CagFqGHY4E7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- based on word2vec, shallow network. random init doc vvector to predict a sample of words in that document. results in static embeddings, treats all context words equally regardless of order/sposition. BERT is bidirection (processes words right and left), is deep, finetuend on sentence similarity, higher scores in benchmakrs,"
      ],
      "metadata": {
        "id": "lOVOLHMh4uqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"I love machine learning.\",\n",
        "    \"Gensim is a useful library for NLP.\",\n",
        "    \"Doc2Vec is an extension of Word2Vec.\",\n",
        "    \"This is an example document.\"\n",
        "]\n",
        "\n",
        "# Preprocess and tag the documents\n",
        "tagged_documents = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
        "\n",
        "# Initialize and train the Doc2Vec model\n",
        "model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=4, epochs=100)\n",
        "\n",
        "# Build the vocabulary\n",
        "model.build_vocab(tagged_documents)\n",
        "\n",
        "# Train the model\n",
        "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Inference: Get the vector for a new document\n",
        "new_doc = \"Machine learning is fascinating.\"\n",
        "new_vec = model.infer_vector(new_doc.split())\n",
        "\n",
        "# Print the vector\n",
        "print(f\"Vector for the new document: {new_vec}\")\n"
      ],
      "metadata": {
        "id": "gYpLIHLsfAqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy"
      ],
      "metadata": {
        "id": "jaP_tMYU-gE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DSqtPdQ4-6Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# brief chatgpt example:\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the SpaCy English model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example set of documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "    \"This is not a document.\",\n",
        "    \"We have more documents to explore.\"\n",
        "]\n",
        "\n",
        "# Function to convert documents to vectors\n",
        "def document_vectors(documents):\n",
        "    doc_vectors = []\n",
        "    for doc in documents:\n",
        "        doc_vector = nlp(doc).vector\n",
        "        doc_vectors.append(doc_vector)\n",
        "    return np.array(doc_vectors)\n",
        "\n",
        "# Convert documents to vectors\n",
        "doc_vectors = document_vectors(documents)\n",
        "\n",
        "# Test document\n",
        "test_doc = \"More documents should be explored.\"\n",
        "\n",
        "# Convert test document to vector\n",
        "test_doc_vector = nlp(test_doc).vector\n",
        "\n",
        "# Calculate cosine similarity between test document and each document in the set\n",
        "similarities = cosine_similarity([test_doc_vector], doc_vectors)[0]\n",
        "\n",
        "# Retrieve indices of most similar documents\n",
        "most_similar_indices = similarities.argsort()[:-4:-1]\n",
        "\n",
        "# Print most similar documents\n",
        "print(\"Most similar documents to the test document:\")\n",
        "for idx in most_similar_indices:\n",
        "    print(f\"Document {idx}: {documents[idx]}\")\n"
      ],
      "metadata": {
        "id": "EBaOuy9v-hui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.dep_, token.tag_, token.has_vector, token.vector_norm)\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "sWiq7Fdu-hsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
        "doc2 = nlp(\"Fast food tastes very good.\")\n",
        "\n",
        "# Similarity of two documents\n",
        "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
        "# Similarity of tokens and spans\n",
        "french_fries = doc1[2:4]\n",
        "burgers = doc1[5]\n",
        "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))\n"
      ],
      "metadata": {
        "id": "P-5oXL2u-hqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misc"
      ],
      "metadata": {
        "id": "64Rgzdm2eTrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "    return similarity\n",
        "\n",
        "# Example vectors\n",
        "vec1 = np.array([1, 2, 3])\n",
        "vec2 = np.array([4, 5, 6])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity = cosine_similarity(vec1, vec2)\n",
        "print(f\"Cosine Similarity: {similarity}\")\n"
      ],
      "metadata": {
        "id": "ycey4uqU0tUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter('A quick brown fox jumps')\n",
        "\n",
        "Counter(['Abc bcg', 'def'])"
      ],
      "metadata": {
        "id": "YMnKymUmeHZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK Stuff"
      ],
      "metadata": {
        "id": "eWiz7u1_3Y2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "puncs = set(string.punctuation)\n",
        "print(puncs)"
      ],
      "metadata": {
        "id": "Mi1rmXY64U20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "0Nmk7ETcrFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) # can also just specify in sklearn\n",
        "len(stop_words)"
      ],
      "metadata": {
        "id": "jp7vWTKl5un_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "8lZl8Vvf3a_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(doc):\n",
        "  tokens = [lemmatizer.lemmatize(word.lower()) for word in doc.split(' ') if word not in stop_words and word not in puncs]\n",
        "  return tokens\n",
        "\n",
        "tokenized_docs = [preprocess(doc) for doc in docs]\n",
        "print(tokenized_docs)"
      ],
      "metadata": {
        "id": "XVabJuxw3a94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since already tokenized, if wanna create BoW, can use:\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False, smooth_idf=True, stop_words=\"english\")\n",
        "tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n",
        "\n",
        "tfidf_matrix.toarray().shape\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "id": "grNrPvfJ3a72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "# for better handling of ambiguous situations like U.S.A.\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Natural language processing is an exciting area. Huge budget have been allocated for this.\"\n",
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "WkSQfyIV69Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "# Find all parts of speech in above sentence\n",
        "tagged = pos_tag(word_tokenize(text))\n",
        "tagged"
      ],
      "metadata": {
        "id": "CNS3KQ8HNivU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Trees"
      ],
      "metadata": {
        "id": "WZQ8uNptcaKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# empty grammar example\n",
        "\n",
        "# Extract all parts of speech from any text\n",
        "chunker = RegexpParser(\"\"\"\n",
        "                    NP: {}   # To extract Noun Phrases\n",
        "                    P: {}    # To extract Prepositions\n",
        "                    V: {}    # To extract Verbs\n",
        "                    PP: {}   # To extract Prepositional Phrases\n",
        "                    VP: {}   # To extract Verb Phrases\n",
        "                    \"\"\")\n",
        "\n",
        "# Print all parts of speech in above sentence\n",
        "output = chunker.parse(tagged)\n",
        "print(\"After Extracting\\n\", output)\n",
        "\n",
        "\n",
        "# output.draw() display error\n",
        "display(output)\n",
        "\n",
        "# another set of grammar rules:\n",
        "\n",
        "#Extract all parts of speech from any text\n",
        "chunker = RegexpParser(\"\"\"\n",
        "                    NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
        "                    P: {<IN>}            #To extract Prepositions\n",
        "                    V: {<V.*>}           #To extract Verbs\n",
        "                    PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
        "                    VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
        "                    \"\"\")\n",
        "\n",
        "# Print all parts of speech in above sentence\n",
        "output = chunker.parse(tagged)\n",
        "print(\"After Extracting\\n\", output)\n",
        "\n",
        "# output.draw() display error\n",
        "display(output)"
      ],
      "metadata": {
        "id": "ihFxRklMNXmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import CFG\n",
        "\n",
        "# CFGs can not be used with english as its not as expressive.\n",
        "\n",
        "# Define a context-free grammar\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "    S -> NP VP\n",
        "    NP -> Det N | 'I'\n",
        "    VP -> V NP | V\n",
        "    Det -> 'the' | 'a'\n",
        "    N -> 'dog' | 'cat'\n",
        "    V -> 'chased' | 'ate'\n",
        "\"\"\")\n",
        "\n",
        "# Create a recursive descent parser\n",
        "parser = nltk.RecursiveDescentParser(grammar)\n",
        "\n",
        "# Define a sentence to parse\n",
        "sentence = \"I chased the cat\"\n",
        "\n",
        "# Parse the sentence and print the parse trees\n",
        "for tree in parser.parse(sentence.split()):\n",
        "    print(tree)\n",
        "    display(tree)\n"
      ],
      "metadata": {
        "id": "buLE6Z7ZMxXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_syntactic_analysis.htm\n",
        "2. https://www.analyticsvidhya.com/blog/2022/03/syntactical-parsing-in-nlp/\n",
        "3. https://intellipaat.com/blog/what-is-parsing-in-nlp/"
      ],
      "metadata": {
        "id": "ExlT62TJcSeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch"
      ],
      "metadata": {
        "id": "eM2KJwbrdAoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4LjM_q4_Pl0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([[1, 2, 3], [5, 2, 5]])\n",
        "t"
      ],
      "metadata": {
        "id": "ouPBT0I8XP0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.t()"
      ],
      "metadata": {
        "id": "I_TqwXs4XPxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.shape\n",
        "\n",
        "un_t = torch.unsqueeze(t, dim=0) # equivalent of np.expand(x, axis=0)\n",
        "un_t"
      ],
      "metadata": {
        "id": "YBHdVErTD3Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un_t.squeeze()"
      ],
      "metadata": {
        "id": "ZqTRarNhD3Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(un_t).item()"
      ],
      "metadata": {
        "id": "anUwsI4cD2_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un_t.view(-1, 1, 1) # only one dimension can be inferred"
      ],
      "metadata": {
        "id": "qhb5t_GVEK-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(2, 3, 5)"
      ],
      "metadata": {
        "id": "o35wGkAPEK7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(t) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(t, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "id": "ZiFToM1xNysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([[1, 1], [1, 1]])\n",
        "\n",
        "# This computes the matrix multiplication between two tensors. y1, y2 will have the same value\n",
        "y1 = t @ t.T\n",
        "y2 = t.matmul(t.T)\n",
        "y1\n",
        "y2\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = t * t\n",
        "z2 = t.mul(t)\n",
        "z1\n",
        "z2"
      ],
      "metadata": {
        "id": "zN7GHbOPOKi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.view(1,-1)"
      ],
      "metadata": {
        "id": "HeLLqjZ4P0pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.view(1,-1).squeeze()\n",
        "n = t.view(1,-1).squeeze().numpy()\n",
        "np.dot(n, n)"
      ],
      "metadata": {
        "id": "8z0AXWvsOKgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t\n",
        "torch.cat((t, t, t), dim=1)"
      ],
      "metadata": {
        "id": "YTj6M4PEHe6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t\n",
        "t[-1, 0:2]"
      ],
      "metadata": {
        "id": "yMBpdFt8He3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(torch.tensor([[1, 2, 0, -1]]) == 1) # masking"
      ],
      "metadata": {
        "id": "tdjIAB-ofPj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sz=3\n",
        "mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
        "mask"
      ],
      "metadata": {
        "id": "NkfIpAONfAuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, 10)"
      ],
      "metadata": {
        "id": "03yrPTOGfAsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [1, 2, 3]\n",
        "np.where(x == 1)[0]"
      ],
      "metadata": {
        "id": "PvFeixU0fAnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sigmoid(torch.tensor(5))\n",
        "\n",
        "torch.nn.ReLU()(torch.tensor(-4))"
      ],
      "metadata": {
        "id": "uITKOw5AHe03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(2, 5)\n",
        "X"
      ],
      "metadata": {
        "id": "vuPq3klXKGee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = torch.nn.Softmax(dim=-1)(torch.tensor([[9], [10], [5]], dtype=float))\n",
        "probs\n",
        "\n",
        "probs.argmax(0)"
      ],
      "metadata": {
        "id": "3Y6R13IQJnCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# an ordered container\n",
        "import torch.nn as nn\n",
        "\n",
        "img_size = 28\n",
        "\n",
        "# no need to define forward()\n",
        "seq_modules = nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(img_size * img_size, 10)\n",
        ")\n",
        "\n",
        "# 5 is probably channels I think\n",
        "input_image = torch.rand(5,img_size,img_size)\n",
        "logits = seq_modules(input_image)\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "yiPbaCPsLB4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = nn.Linear(4, 1)\n",
        "for x in linear_layer.named_parameters():\n",
        "  print('---------')\n",
        "  print(x[0], x[1].numel())"
      ],
      "metadata": {
        "id": "qnoCbcKkGwAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    nn.Linear(3, 2),\n",
        "    nn.Linear(2, 1)\n",
        ")\n",
        "for x in seq_modules.named_parameters():\n",
        "  print('---------')\n",
        "  print(x[0], x[1].numel())"
      ],
      "metadata": {
        "id": "Jx-hRXgCHqe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# can use within collate_fn in dataloader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Example list of sequences with varying lengths\n",
        "sequences = [torch.tensor([1, 2, 3], dtype=torch.float),\n",
        "             torch.tensor([4, 5], dtype=torch.float),\n",
        "             torch.tensor([6], dtype=torch.float)]\n",
        "\n",
        "# Pad the sequences\n",
        "padded_sequence = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
        "padded_sequence"
      ],
      "metadata": {
        "id": "1X4qRbMiXaOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# serves different purpose.\n",
        "# ignores pad values and leads to better memory utilization\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "lengths = torch.tensor([len(t) for t in padded_sequence])\n",
        "\n",
        "# Pack the padded sequence\n",
        "packed_sequence = pack_padded_sequence(padded_sequence, lengths, batch_first=True)\n",
        "packed_sequence"
      ],
      "metadata": {
        "id": "C31KE7KNXPvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "# can't use tensordataset directly for variable input lengths\n",
        "# need to implement class\n",
        "\n",
        "# can't init tensor on variable. otherwise, just init as tensor and use tensordataset\n",
        "X = [[1, 2, 3], [5, 7], [5]]\n",
        "\n",
        "# needed\n",
        "X = list(map(torch.tensor, X))\n",
        "\n",
        "y = torch.tensor([0, 1, 1])\n",
        "\n",
        "# can't use for variable\n",
        "#dataset = TensorDataset(X, y)\n",
        "\n",
        "class SequenceDataset(Dataset): # just a protocol class so doesn't inherit anythinmg no super()\n",
        "  def __init__(self, sequences, labels):\n",
        "    self.sequences = sequences\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "dataset = SequenceDataset(X, y)\n",
        "\n",
        "def collate_fn(batch):\n",
        "  sequences, labels = zip(*batch)\n",
        "  lengths = [len(seq) for seq in sequences]\n",
        "  padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "  return padded, lengths, labels\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "# Iterate through DataLoader\n",
        "for batch in dataloader:\n",
        "    padded_sequences, lengths, labels = batch\n",
        "    print(\"Padded Sequences:\\n\", padded_sequences)\n",
        "    print(\"Lengths:\\n\", lengths)\n",
        "    print(\"Labels:\\n\", labels)"
      ],
      "metadata": {
        "id": "zBg1bgVGftR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming Data"
      ],
      "metadata": {
        "id": "xWAvUqpsM2Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can also use pandas(chunk_size) or maybe using numpy.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MCpkyRlLM9zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "file = 'example6.h5'\n",
        "\n",
        "# Create an HDF5 file\n",
        "with h5py.File(file, 'w') as f:\n",
        "    f.create_dataset('data', data=np.arange(100), compression=\"gzip\")\n",
        "    f.create_dataset('labels', data=np.random.randint(0, 3, size=(100,)), compression=\"gzip\")\n",
        "\n",
        "class HDF5Dataset(Dataset):\n",
        "    def __init__(self, h5_file):\n",
        "        self.file = h5_file\n",
        "        self.dataset = h5py.File(h5_file, 'r')\n",
        "        self.data = self.dataset['data']\n",
        "        self.labels = self.dataset['labels']\n",
        "        #self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# Usage example\n",
        "h5_file = file\n",
        "dataset = HDF5Dataset(h5_file)"
      ],
      "metadata": {
        "id": "5SuvxbxOfAcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Iterate over DataLoader in training loop\n",
        "for inputs, labels in data_loader:\n",
        "    # Perform training step\n",
        "    print(inputs)\n",
        "    print('===========')"
      ],
      "metadata": {
        "id": "jeNRRLuBfAaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CBOW in PyTroch"
      ],
      "metadata": {
        "id": "03T4Ex22F-YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "corpus = [\n",
        "    \"he is the king\",\n",
        "    \"the king is royal\",\n",
        "    \"she is the queen\",\n",
        "    \"the queen is royal\",\n",
        "    \"royal is nice\",\n",
        "    \"she is nice\"\n",
        "]\n",
        "\n",
        "# Tokenize corpus\n",
        "words = set()\n",
        "for sentence in corpus:\n",
        "    for word in sentence.split():\n",
        "      words.add(word)\n",
        "\n",
        "#print([word for sent in corpus for word in sent.split(' ')])"
      ],
      "metadata": {
        "id": "LR4i6lXPGlbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word_counts = Counter(words)\n",
        "vocab = {word: idx for idx, word in enumerate(words)}\n",
        "idx_to_word = {idx: word for word, idx in vocab.items()}"
      ],
      "metadata": {
        "id": "wDDB6ndvHRHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CBOW Dataset\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, corpus, window_size=2):\n",
        "        self.corpus = corpus\n",
        "        self.window_size = window_size\n",
        "        self.data = self.generate_data()\n",
        "\n",
        "    def generate_data(self):\n",
        "        data = []\n",
        "        for sentence in self.corpus:\n",
        "            words = sentence.split()\n",
        "            for i, target_word in enumerate(words):\n",
        "                context = [words[j] for j in range(max(0, i - self.window_size), min(len(words), i + self.window_size + 1)) if j != i]\n",
        "                data.append((context, target_word))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target_word = self.data[idx]\n",
        "        context_idx = [vocab[word] for word in context]\n",
        "        target_idx = vocab[target_word]\n",
        "        return torch.tensor(context_idx), torch.tensor(target_idx)\n",
        "\n",
        "# CBOW Model\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).mean(dim=1)  # Average embedding of context words\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NcxmHJQzIWNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 5\n",
        "window_size = 2\n",
        "batch_size = 4\n",
        "lr = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CBOWDataset(corpus, window_size=window_size)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = CBOW(vocab_size, embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "WudfZa7FJVbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "Eq1yg7-VLVzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding.weight"
      ],
      "metadata": {
        "id": "qnfpRETDLS7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word embeddings\n",
        "word_embeddings = model.embedding.weight.detach().numpy()\n",
        "\n",
        "# Print word embeddings\n",
        "for idx, word in idx_to_word.items():\n",
        "    print(f\"{word}: {word_embeddings[idx]}\")\n"
      ],
      "metadata": {
        "id": "M9TmKFVVi1j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression with RNN"
      ],
      "metadata": {
        "id": "8XV2iXkJbNo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example data: a sequence of numbers\n",
        "data = torch.arange(1, 11, dtype=torch.float32).view(-1, 1)\n",
        "sequence_length = 3  # Length of each sequence\n",
        "data"
      ],
      "metadata": {
        "id": "ts8SDTJebZEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_inout_sequences(input_data, seq_length):\n",
        "    inout_seq = []\n",
        "    L = len(input_data)\n",
        "    for i in range(L-seq_length):\n",
        "        seq = input_data[i:i+seq_length]\n",
        "        label = input_data[i+seq_length:i+seq_length+1]\n",
        "        inout_seq.append((seq, label))\n",
        "    return inout_seq\n",
        "\n",
        "seq_data = create_inout_sequences(data, sequence_length)\n",
        "for seq, labels in seq_data:\n",
        "  print(seq, labels)\n",
        "  break"
      ],
      "metadata": {
        "id": "QpUmbMTLbZCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=10, output_size=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # Initial hidden state\n",
        "\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Fully connected layer on the last time step\n",
        "        # according to chatgpt\n",
        "        # this equals\n",
        "        # out = self.fc(hn.squeeze(0))  # Using hn instead of out[:, -1, :]\n",
        "        # i dont think its true for bidirectional coz output has last layer only (all t).\n",
        "        # h as last time step (both layers)\n",
        "\n",
        "        # a single number\n",
        "        return out\n",
        "\n",
        "model = SimpleRNN()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Train the Model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for seq, labels in seq_data:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(seq.unsqueeze(0))  # Add batch dimension\n",
        "        loss = criterion(y_pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "iBLRk1Owbu0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Generation with RNN"
      ],
      "metadata": {
        "id": "Hgy9MNlrNerP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Define constants\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "BATCH_SIZE = 2\n",
        "NUM_EPOCHS = 10\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "# Define dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Function to build vocabulary and tokenizer\n",
        "def build_vocab_and_tokenizer(data):\n",
        "    words = [word for sentence in data for word in sentence.split()]\n",
        "    word_counts = Counter(words)\n",
        "    vocab = [word for word, count in word_counts.items()]\n",
        "    vocab.append(START_TOKEN)\n",
        "    vocab.append(END_TOKEN)\n",
        "    vocab.append(PAD_TOKEN)\n",
        "    tokenizer = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, tokenizer\n",
        "\n",
        "# Function to convert text to tensor indices\n",
        "def text_to_tensor(text, tokenizer):\n",
        "    tensor = [tokenizer.get(word, tokenizer[PAD_TOKEN]) for word in text.split()]\n",
        "    return tensor\n",
        "\n",
        "# Custom collate function to handle variable length sequences\n",
        "def collate_fn(batch):\n",
        "    texts = [item for item in batch]\n",
        "    tensor_texts = [torch.LongTensor(text_to_tensor(text, tokenizer)) for text in texts]\n",
        "    padded_texts = pad_sequence(tensor_texts, batch_first=True, padding_value=tokenizer[PAD_TOKEN])\n",
        "    return padded_texts\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('Batch size: ', x.size())\n",
        "        embedded = self.embedding(x)\n",
        "        print('Embeddings size: ', embedded.size())\n",
        "\n",
        "        output, _ = self.rnn(embedded)\n",
        "        print('Output: ', output.size())\n",
        "        output = self.fc(output)\n",
        "        print('Final size: ', output.size())\n",
        "\n",
        "        # i think outputting all because we are modelling language at each step\n",
        "        return output\n",
        "\n",
        "# Prepare data\n",
        "data = docs.copy() # from above\n",
        "\n",
        "# Split data into train and test\n",
        "random.shuffle(data)\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data = data[:train_size]\n",
        "test_data = data[train_size:]\n",
        "\n",
        "# Build vocabulary and tokenizer\n",
        "vocab, tokenizer = build_vocab_and_tokenizer(train_data)\n",
        "\n",
        "# Define datasets and dataloaders\n",
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = RNNLanguageModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch[:, :-1])  # Ignore the last token\n",
        "        target = batch[:, 1:]  # Predict next token\n",
        "\n",
        "        # all tokens of entire batch contribute to loss\n",
        "        loss = criterion(output.reshape(-1, len(vocab)), target.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation loop\n",
        "model.eval()\n",
        "total_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        output = model(batch[:, :-1])  # Ignore the last token\n",
        "        target = batch[:, 1:]  # Predict next token\n",
        "        loss = criterion(output.reshape(-1, len(vocab)), target.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "print(f\"Test Loss: {total_loss/len(test_loader)}\")\n",
        "\n",
        "# Inference function\n",
        "def generate_text(model, tokenizer, max_length=20):\n",
        "    model.eval()\n",
        "    current_token = tokenizer[START_TOKEN]\n",
        "    generated_text = [current_token]\n",
        "    while len(generated_text) < max_length:\n",
        "        input_tensor = torch.LongTensor([[current_token]])\n",
        "        output = model(input_tensor)\n",
        "        probabilities = torch.softmax(output[0, -1], dim=0)\n",
        "        next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
        "        if next_token == tokenizer[END_TOKEN]:\n",
        "            break\n",
        "        generated_text.append(next_token)\n",
        "        current_token = next_token\n",
        "    return ' '.join([vocab[token] for token in generated_text])\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, tokenizer, max_length=20)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "1dJLAsToNlp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU for Text Generation"
      ],
      "metadata": {
        "id": "AwrqQDO9RVEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "\n",
        "# Define the dataset\n",
        "class LanguageDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "# Function to tokenize the text and build vocabulary\n",
        "def build_vocab(texts):\n",
        "    vocab = set()\n",
        "    for text in texts:\n",
        "        vocab.update(text.split())\n",
        "    vocab = list(vocab)\n",
        "    vocab.append(START_TOKEN)\n",
        "    vocab.append(END_TOKEN)\n",
        "    vocab.append(PAD_TOKEN)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    return vocab, word_to_idx, idx_to_word\n",
        "\n",
        "# Function to encode text into tensor sequence\n",
        "def encode_text(text, word_to_idx):\n",
        "    return torch.tensor([word_to_idx[word] for word in text.split()])\n",
        "\n",
        "# Function to decode tensor sequence into text\n",
        "def decode_sequence(sequence, idx_to_word):\n",
        "    return ' '.join([idx_to_word[idx.item()] for idx in sequence])\n",
        "\n",
        "# Define the GRU language model\n",
        "class GRULanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(GRULanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.gru(embedded)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Function to handle variable length sequences in DataLoader\n",
        "def collate_fn(batch):\n",
        "    texts = [item for item in batch]\n",
        "    encoded_texts = [encode_text(text, word_to_idx) for text in texts]\n",
        "    padded_texts = pad_sequence(encoded_texts, batch_first=True, padding_value=PAD_IDX)\n",
        "    return padded_texts\n",
        "\n",
        "# Define training and evaluation loop\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.permute(0, 2, 1), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
        "\n",
        "# Define generator function for inference\n",
        "def generate_text(model, start_token_idx, end_token_idx, max_length):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        current_token = torch.tensor([start_token_idx]).unsqueeze(0)\n",
        "        generated_text = []\n",
        "        while len(generated_text) < max_length:\n",
        "            output_probs = model(current_token)\n",
        "\n",
        "            # use logits of the last elemet in the sequence\n",
        "            _, next_token = torch.max(output_probs[:, -1, :], dim=1)\n",
        "            if next_token.item() == end_token_idx:\n",
        "                break\n",
        "            generated_text.append(next_token.item())\n",
        "            current_token = torch.cat((current_token, next_token.unsqueeze(0)), dim=1)\n",
        "        return generated_text\n",
        "\n",
        "# Example usage\n",
        "texts = docs.copy()#[\"This is an example text.\", \"Another example text.\"]\n",
        "vocab, word_to_idx, idx_to_word = build_vocab(texts)\n",
        "PAD_IDX = len(vocab)\n",
        "vocab_size = len(vocab) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "start_token_idx = vocab.index('<START>')\n",
        "end_token_idx = vocab.index('<END>')\n",
        "max_length = 5\n",
        "\n",
        "dataset = LanguageDataset(texts)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model = GRULanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "generated_text = generate_text(model, start_token_idx, end_token_idx, max_length)\n",
        "print(decode_sequence(torch.tensor(generated_text), idx_to_word))\n"
      ],
      "metadata": {
        "id": "k6XCWJRCNllT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another GRU for Text Generation (duplicated)"
      ],
      "metadata": {
        "id": "wAG2HbNwR9iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Define your dataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # no labels coz self-supervised\n",
        "        return self.data[idx]\n",
        "\n",
        "# Define your model architecture\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        print('Embedding size: ', embedded.size())\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # 1 = n_layers\n",
        "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "# Define your data processing functions\n",
        "def build_vocab(data):\n",
        "    word_counter = Counter()\n",
        "    for sentence in data:\n",
        "        word_counter.update(sentence.split())\n",
        "    vocab = ['<pad>', '<start>', '<end>', '<unk>'] + [word for word, _ in word_counter.most_common()]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return vocab, word2idx, idx2word\n",
        "\n",
        "def encode_sentence(sentence, word2idx):\n",
        "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences = [torch.LongTensor(item) for item in batch]\n",
        "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "    return padded_sentences\n",
        "\n",
        "# Define your training loop\n",
        "def train(model, train_loader, criterion, optimizer, batch_size, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[:, :-1].to(device)\n",
        "        targets = batch[:, 1:].to(device)\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs, hidden)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(2)), targets.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Define your evaluation loop\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch[:, :-1].to(device)\n",
        "            targets = batch[:, 1:].to(device)\n",
        "            hidden = model.init_hidden(len(batch))\n",
        "            outputs, _ = model(inputs, hidden)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(2)), targets.contiguous().view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "# Define your inference function\n",
        "def generate_sentence(model, start_token_idx, end_token_idx, max_length, batch_size, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        current_token_idx = start_token_idx\n",
        "        sentence = [current_token_idx]\n",
        "        hidden = model.init_hidden(1)\n",
        "        for _ in range(max_length):\n",
        "            inputs = torch.LongTensor([[current_token_idx]]).to(device)\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            predicted_token_idx = outputs.argmax().item()\n",
        "            if predicted_token_idx == end_token_idx:\n",
        "                break\n",
        "            sentence.append(predicted_token_idx)\n",
        "            current_token_idx = predicted_token_idx\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "z-gY8lQFR8yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "#data = ['<start> hello how are you <end>', '<start> i am fine thank you <end>']\n",
        "data = docs.copy()\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "vocab, word2idx, idx2word = build_vocab(train_data)\n",
        "train_data_encoded = [encode_sentence(sentence, word2idx) for sentence in train_data]\n",
        "val_data_encoded = [encode_sentence(sentence, word2idx) for sentence in val_data]\n",
        "\n",
        "train_dataset = MyDataset(train_data_encoded)\n",
        "val_dataset = MyDataset(val_data_encoded)\n",
        "\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "O_U7UJIhVzcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MyModel(vocab_size=len(vocab), embed_size=50, hidden_size=100)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "7hTd2YaTVssj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, batch_size, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "# Example inference\n",
        "start_token_idx = word2idx['<start>']\n",
        "end_token_idx = word2idx['<end>']\n",
        "generated_sentence = generate_sentence(model, start_token_idx, end_token_idx, 10, batch_size, device)\n",
        "generated_sentence = ' '.join([idx2word[idx] for idx in generated_sentence])\n",
        "print(generated_sentence)"
      ],
      "metadata": {
        "id": "F59091A_Vspr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM for Sentiment Analysis"
      ],
      "metadata": {
        "id": "Zs37MjfFKfSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# IMDb reviews data as a dictionary\n",
        "imdb_reviews = {\n",
        "    \"review\": [\n",
        "        \"The Shawshank Redemption is an incredible movie. The acting is superb and the storyline is captivating.\",\n",
        "        \"I found the plot to be confusing and the acting to be mediocre. Overall, I was disappointed with the film.\",\n",
        "        \"Great movie! The special effects were amazing and the characters were well-developed.\",\n",
        "        \"The dialogue felt forced and unrealistic. I couldn't connect with any of the characters.\",\n",
        "        \"The Godfather is a masterpiece. The performances are outstanding and the direction is flawless.\",\n",
        "        \"The pacing of the film was too slow, and I found myself losing interest halfway through.\",\n",
        "        \"Titanic is a classic love story that will tug at your heartstrings. I highly recommend it.\",\n",
        "        \"The acting was wooden and the plot was predictable. I was not impressed.\",\n",
        "        \"Jurassic Park is a thrilling adventure that keeps you on the edge of your seat from start to finish.\",\n",
        "        \"I felt the movie lacked depth and the characters were one-dimensional. Overall, it was underwhelming.\"\n",
        "    ],\n",
        "    \"sentiment\": [\n",
        "        \"positive\",\n",
        "        \"negative\",\n",
        "        \"positive\",\n",
        "        \"negative\",\n",
        "        \"positive\",\n",
        "        \"negative\",\n",
        "        \"positive\",\n",
        "        \"negative\",\n",
        "        \"positive\",\n",
        "        \"negative\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(imdb_reviews)\n",
        "df = pd.concat([df, df]).reset_index(drop=True)\n",
        "df"
      ],
      "metadata": {
        "id": "eXpQALyWdB6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df['review'], df['sentiment']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# can also use random_split() by torch on top of pytorch's dataset\n",
        "# like: train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "3A7tNkD7dB4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "WP8H2ZL5dB2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import string\n",
        "\n",
        "def preprocess_word(w):\n",
        "    return w.lower()\n",
        "\n",
        "def build_vocab(X_train):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_list = set()\n",
        "\n",
        "    # Building the vocabulary\n",
        "    for sent in X_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_word(word)\n",
        "            if word not in stop_words and word != '' and word not in string.punctuation:\n",
        "                word_list.add(word)\n",
        "\n",
        "    vocab = {w: i for i, w in enumerate(word_list)}\n",
        "    return vocab\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokenized_list = []\n",
        "    for sent in sentences:\n",
        "        tokenized_words = [vocab[preprocess_word(w)] for w in sent.lower().split() if preprocess_word(w) in vocab]\n",
        "        if len(tokenized_words) == 0:\n",
        "          print(sent)\n",
        "          continue\n",
        "        tokenized_list.append(torch.tensor(tokenized_words))\n",
        "    return tokenized_list\n",
        "\n",
        "def encode_labels(labels):\n",
        "  return torch.tensor([1 if label == 'positive' else 0 for label in labels])\n",
        "\n",
        "\n",
        "def preprocess(X, y):\n",
        "    tokenized_text = tokenize(X.values)\n",
        "    encoded_labels = encode_labels(y.values)\n",
        "    return tokenized_text, encoded_labels\n",
        "\n",
        "vocab = build_vocab(X_train)\n",
        "\n",
        "# Tokenize/encode\n",
        "X_train_tokenized, y_train_tokenized = preprocess(X_train, y_train)\n",
        "X_test_tokenized, y_test_tokenized = preprocess(X_test, y_test)\n",
        "\n",
        "print(X_train_tokenized)\n",
        "print(y_train_tokenized)\n",
        "print(X_test_tokenized)\n",
        "print(y_test_tokenized)"
      ],
      "metadata": {
        "id": "7G1H_gL2tL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "# create Tensor datasets\n",
        "\n",
        "# using custom class for variable input\n",
        "train_data = SequenceDataset(X_train_tokenized, y_train_tokenized)\n",
        "valid_data = SequenceDataset(X_test_tokenized, y_test_tokenized)\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 2\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, labels = zip(*batch)\n",
        "    lengths = [len(sent) for sent in sentences]\n",
        "    padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
        "    return padded, torch.tensor(lengths), torch.tensor(labels)\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_lengths, sample_y = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print('Sample output: \\n', sample_y)\n",
        "print('Sample sample_lengths: \\n', sample_lengths)"
      ],
      "metadata": {
        "id": "sBPixNFHdBvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "_xwlossk_-Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(torch.nn.Module):\n",
        "    def __init__(self, no_layers, vocab_size,hidden_dim,embedding_dim,output_dim,drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #lstm: DOES NOT DEPEND ON SEQUENCE LENGTH. SHARED WEIGHTS.\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=no_layers,\n",
        "            batch_first=True #bidirectional=True\n",
        "        )\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = torch.nn.Linear(self.hidden_dim, output_dim) # *2 because of bidirectional\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, lengths, hidden):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "\n",
        "        # using packed sequences\n",
        "        packed_embed = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # handles feedback loop and processes all the tokens in a sequence\n",
        "        lstm_out, hidden = self.lstm(packed_embed, hidden)\n",
        "\n",
        "        # for BILSTM: concatenate the final forward and backward hidden states\n",
        "        # forward even indices, backward odd`\n",
        "        # hidden_concat = torch.cat((hidden[0][-2, :, :], hidden[0][-1, :, :]), dim=1)\n",
        "\n",
        "        # Unpack the PackedSequence\n",
        "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        \"\"\"\n",
        "        Reshapes the lstm_out tensor to have shape (batch_size * sequence_length, hidden_dim).\n",
        "        The -1 is a placeholder that infers the appropriate size automatically based on the\n",
        "        other dimension (hidden_dim). This essentially flattens the tensor so that each time step\n",
        "        for each batch item is treated as an independent sample.\n",
        "        \"\"\"\n",
        "\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        sig_out = sig_out[:, -1] # get element of each batch I guess\n",
        "\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "\n",
        "        # twice layers for BILSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "\n",
        "    # optionally:\n",
        "    # def init_weights(self):\n",
        "    #     for param in self.parameters():\n",
        "    #         nn.init.kaiming_uniform_(param, a=math.sqrt(5))"
      ],
      "metadata": {
        "id": "e5G35IyndBtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 # extra 1 for padding????\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 12\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim, output_dim, drop_prob=0.5)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "3-vOR2JzBBr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()}\\n\")"
      ],
      "metadata": {
        "id": "NeS8f0YaLiGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate correct labels\n",
        "def acc(pred,label):\n",
        "    # simple way to convert probs into classes\n",
        "    pred = torch.round(pred.squeeze())\n",
        "\n",
        "    # count correct predictions\n",
        "    return torch.sum(pred == label.squeeze()).item()"
      ],
      "metadata": {
        "id": "wWwZs9nzBBpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = 5 # gradient clipping for exploding gradients\n",
        "epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "\n",
        "    # layers like dropout are enabled. gradients ar eclcualed. batch normalziation uses batch statistics.\n",
        "    model.train()\n",
        "\n",
        "    for inputs, lengths, labels in train_loader:\n",
        "        # move to GPU\n",
        "        inputs, lengths, labels = inputs.to(device), lengths.to(device), labels.to(device)\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise we'd backprop through the entire training history\n",
        "        # h = tuple([each.data for each in h])\n",
        "\n",
        "        # why not just init wihtin the loop\n",
        "        # initialize hidden states and cell states of LSTM\n",
        "        h = model.init_hidden(batch_size)\n",
        "\n",
        "        # clear grad to prevent gradient accumulation across batches.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forward() pass. maintain operations gradient functions in DAG .grad_fn\n",
        "        output, _ = model(inputs, lengths, h)  # h not used outside\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels.float())\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # calculate gradients through backpropagation into .grad, applies chain rule\n",
        "        loss.backward()\n",
        "\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculating accuracy (correct labels)\n",
        "        accuracy = acc(output, labels)\n",
        "        train_acc += accuracy # actually correct predictions\n",
        "\n",
        "\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "\n",
        "    # disable some operations\n",
        "    model.eval()\n",
        "\n",
        "    # no gradient calculations needed\n",
        "    with torch.no_grad():\n",
        "      for inputs, lengths, labels in valid_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        #val_h = tuple([each.data for each in val_h])\n",
        "        val_h = model.init_hidden(batch_size)\n",
        "\n",
        "        output, val_h = model(inputs, lengths, val_h)\n",
        "\n",
        "        val_loss = criterion(output, labels.float())\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "        accuracy = acc(output,labels)\n",
        "        val_acc += accuracy\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), 'state_dict.pt')\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')\n"
      ],
      "metadata": {
        "id": "l8370CoXE9tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "  model.eval()\n",
        "\n",
        "  word_seq = np.array([vocab[preprocess_word(word)] for word in text.split() if preprocess_word(word) in vocab])\n",
        "  length = torch.tensor([len(word_seq)])\n",
        "  word_seq = np.expand_dims(word_seq, axis=0)\n",
        "  inputs = torch.from_numpy(word_seq).to(device)\n",
        "  batch_size = 1\n",
        "  h = model.init_hidden(batch_size)\n",
        "  # h = tuple([each.data for each in h]) # maybe unnecessary\n",
        "  with torch.no_grad():\n",
        "    output, h = model(inputs, length, h)\n",
        "  probability = output.item()\n",
        "  return probability\n",
        "\n",
        "index = 0\n",
        "predict_text('this movie sucks.')"
      ],
      "metadata": {
        "id": "hnFpZq2xE9qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "id": "xnbpvCZ6f9Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN for Sentiment Analysis"
      ],
      "metadata": {
        "id": "0anGERTo_JRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "texts = [\n",
        "    \"I love this movie\",\n",
        "    \"This film was terrible\",\n",
        "    \"Absolutely fantastic\",\n",
        "    \"Worst movie ever\",\n",
        "    \"I enjoyed it\",\n",
        "    \"Not my favorite\",\n",
        "    \"Best movie I have seen\",\n",
        "    \"Could be better\",\n",
        "    \"I would watch it again\",\n",
        "    \"Awful experience\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
        "\n",
        "# Train-test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization\n",
        "tokenized_train_texts = [text.split() for text in train_texts]\n",
        "\n",
        "# Vocabulary creation\n",
        "train_vocab = Counter(itertools.chain(*tokenized_train_texts))\n",
        "train_vocab = {word: i + 2 for i, (word, _) in enumerate(train_vocab.most_common())}  # Start indices from 2\n",
        "\n",
        "# I don't think this info is needed for tasks other than langauge modelling\n",
        "train_vocab['<PAD>'] = 0\n",
        "train_vocab['<UNK>'] = 1\n",
        "\n",
        "# Convert texts to sequences of indices\n",
        "def text_to_sequence(text, vocab):\n",
        "    return [vocab.get(word, vocab['<UNK>']) for word in text]\n",
        "\n",
        "# Padding\n",
        "def pad_sequence(seq, max_len):\n",
        "    return seq + [train_vocab['<PAD>']] * (max_len - len(seq))"
      ],
      "metadata": {
        "id": "5TmbSZ8hJw6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.data = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    print('Batch: ')\n",
        "    print(batch)\n",
        "    print('Unpacked:')\n",
        "    print(*batch)\n",
        "    print('Zipped:')\n",
        "    print(zip(*batch))\n",
        "    texts, labels = zip(*batch)\n",
        "    sequences = [pad_sequence(text, max_len) for text in texts]\n",
        "    return torch.tensor(sequences, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_sequences = [text_to_sequence(text, train_vocab) for text in tokenized_train_texts]\n",
        "max_len = max(len(seq) for seq in train_sequences)\n",
        "train_dataset = SentimentDataset(train_sequences, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "for batch in train_dataloader:\n",
        "  tokens, labels = batch\n",
        "  print(tokens, labels)\n",
        "  break"
      ],
      "metadata": {
        "id": "HJTtNT14J07P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # treat embedding dimension as in_channels, kernel size is just a window for context words\n",
        "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=50, kernel_size=3)  # 50 filters, kernel size 3\n",
        "        self.fc = nn.Linear(50 * (max_len - 3 + 1), num_classes)  # Fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, max_len)\n",
        "        x = self.embedding(x)  # (batch_size, max_len, embed_dim)\n",
        "        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, embed_dim, max_len)\n",
        "        x = F.relu(self.conv(x))  # Apply convolution: (batch_size, 50, max_len-3+1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 50 * (max_len-3+1))\n",
        "        x = self.fc(x)  # Fully connected layer: (batch_size, num_classes)\n",
        "        return x\n",
        "\n",
        "vocab_size = len(train_vocab)\n",
        "embed_dim = 50  # Reduced embedding dimension for simplicity\n",
        "num_classes = 2\n",
        "\n",
        "model = SimpleCNNModel(vocab_size, embed_dim, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    print('==='*50)\n",
        "    for batch_data, batch_labels in train_dataloader:\n",
        "        outputs = model(batch_data)  # logits\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Sample inference\n",
        "sample_text = \"I really love this movie\"\n",
        "sample_sequence = text_to_sequence(sample_text.split(), train_vocab)\n",
        "sample_sequence_padded = pad_sequence(sample_sequence, max_len)\n",
        "sample_data = torch.tensor([sample_sequence_padded], dtype=torch.long)\n",
        "output = model(sample_data)  # logits\n",
        "\n",
        "print(output)\n",
        "\n",
        "_, predicted = torch.max(output, 1)\n",
        "print(f'Predicted: {predicted.item()}')"
      ],
      "metadata": {
        "id": "XEc02VIT_d4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "uDfRnhUZ_Gfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline / Sklearn"
      ],
      "metadata": {
        "id": "TbynZe0D8Ydm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "X = pd.DataFrame(\n",
        "    {'city': ['London', 'London', 'Paris', 'Sallisaw'],\n",
        "     'state' : ['NY', 'NJ', 'ON', 'QB'],\n",
        "     'title': [\"His Last Bow\", \"How Watson Learned the Trick\",\n",
        "               \"A Moveable Feast\", \"The Grapes of Wrath\"],\n",
        "     'expert_rating': [5, 3, 4, 5],\n",
        "     'user_rating': [4, 5, 4, 3]})\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "RYB5Axo269HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "# using columntrasnformer avoids data leakage that happens if we preprocess before splitting/sklearn training.\n",
        "# Also, can be parameterized.\n",
        "\n",
        "column_trans = ColumnTransformer(\n",
        "    [('categories', OneHotEncoder(dtype='int', handle_unknown='ignore'), ['city']), # list for most transformers like this one\n",
        "     ('title_bow', CountVectorizer(), 'title'),\n",
        "     ('standard_scaler', StandardScaler(), make_column_selector(dtype_include=np.number))\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# NOTE:\n",
        "# shouldn't fit before split\n",
        "column_trans.fit(X)\n",
        "\n",
        "column_trans.get_feature_names_out()\n",
        "\n",
        "feature_set = column_trans.transform(X)#.toarray()\n",
        "feature_set"
      ],
      "metadata": {
        "id": "bpPEb5Px8dIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import set_config\n",
        "import joblib\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = make_pipeline(column_trans, RandomForestClassifier())\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X.drop(columns=['user_rating']),\n",
        "    X['user_rating'],\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Save the pipeline for future use\n",
        "joblib.dump(pipeline, 'pipeline.pkl')\n",
        "\n",
        "# Later, load the pipeline and use it to make predictions on new data\n",
        "pipeline_loaded = joblib.load('pipeline.pkl')\n",
        "\n",
        "# Make predictions on new data\n",
        "predictions = pipeline_loaded.predict(X_valid)\n",
        "predictions"
      ],
      "metadata": {
        "id": "yaMpQrVS-vEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# can use cross_val_score for pileine\n",
        "\n",
        "scores = cross_val_score(pipeline, X.drop(columns=['user_rating']), X['user_rating'], cv=2)\n",
        "print(f\"Mean accuracy: {scores.mean():.2f}+/-{scores.std():.2f}\")"
      ],
      "metadata": {
        "id": "bErLizKk8dGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA"
      ],
      "metadata": {
        "id": "eN9R03nzkIG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "embedding_model = \"all-MiniLM-L12-v2\""
      ],
      "metadata": {
        "id": "Cjamqpgpll2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "782BsT8iv-rR"
      },
      "outputs": [],
      "source": [
        "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
        "                                stop_words = 'english',\n",
        "                                lowercase = True,\n",
        "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
        "                                max_df = 0.5,\n",
        "                                min_df = 1)\n",
        "\n",
        "dtm_tf = tf_vectorizer.fit_transform(docs)\n",
        "tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
        "dtm_tfidf = tfidf_vectorizer.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense_matrix = dtm_tf.toarray()\n",
        "dense_matrix"
      ],
      "metadata": {
        "id": "TZSqLl0eNca7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_matrix_tfidf = dtm_tfidf.toarray()\n",
        "print(dense_matrix_tfidf.shape)\n",
        "dense_matrix_tfidf"
      ],
      "metadata": {
        "id": "mkuUfBpcN9yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDxUZISdv-pM"
      },
      "outputs": [],
      "source": [
        "# for TF DTM\n",
        "lda_tf = LatentDirichletAllocation(n_components=4, random_state=0)\n",
        "lda_tf.fit(dtm_tf)\n",
        "\n",
        "# for TFIDF DTM\n",
        "lda_tfidf = LatentDirichletAllocation(n_components=4, random_state=0)\n",
        "lda_tfidf.fit(dtm_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCKyiTWEve41"
      },
      "outputs": [],
      "source": [
        "from pyLDAvis import lda_model as psk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scipy.__version__"
      ],
      "metadata": {
        "id": "_UC_HKnLPA_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "# # Example corpus of documents\n",
        "# corpus = [\n",
        "#     \"This is the first document\",\n",
        "#     \"This document is the second document\",\n",
        "#     \"And this is the third one\",\n",
        "#     \"Is this the first document?\"\n",
        "# ]\n",
        "\n",
        "corpus = docs.copy()\n",
        "\n",
        "# Preprocessing the corpus\n",
        "tokenized_corpus = [document.lower().split() for document in corpus]\n",
        "\n",
        "# Creating the dictionary\n",
        "dictionary = corpora.Dictionary(tokenized_corpus)"
      ],
      "metadata": {
        "id": "35wyJZsXOq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus"
      ],
      "metadata": {
        "id": "y3BHIyW4OrTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary"
      ],
      "metadata": {
        "id": "XidTIK7yOtHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "id": "qRm-v7jeP-sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the document-term-frequency matrix\n",
        "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in tokenized_corpus]\n",
        "doc_term_matrix"
      ],
      "metadata": {
        "id": "AFcFKUHZPUju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLxrzWRZwDy6"
      },
      "outputs": [],
      "source": [
        "# Training the LDA model\n",
        "lda_model = models.LdaModel(\n",
        "    corpus=doc_term_matrix,\n",
        "    id2word=dictionary,\n",
        "    num_topics=3,\n",
        "    passes=10\n",
        ")\n",
        "\n",
        "# Print the topics and their associated keywords\n",
        "for topic_id, topic_keywords in lda_model.print_topics():\n",
        "    print(f\"Topic ID: {topic_id}\")\n",
        "    print(f\"Keywords: {topic_keywords}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.039 + 0.025 + 0.021 + 0.019 + 0.018 + 0.017 + 0.017 + 0.017 + 0.017 + 0.017 + 0.016 + 0.016"
      ],
      "metadata": {
        "id": "RU3Sde2YeMEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilityies of classes\n",
        "count = 0\n",
        "for doc in lda_model[doc_term_matrix]:\n",
        "  print('doc: ', count, doc)\n",
        "  count += 1"
      ],
      "metadata": {
        "id": "A588pKN5SQlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLltgZMQwDvl"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim\n",
        "\n",
        "# # Example corpus of documents\n",
        "# corpus = [\n",
        "#     \"This is the first document\",\n",
        "#     \"This document is the second document\",\n",
        "#     \"And this is the third one\",\n",
        "#     \"Is this the first document?\"\n",
        "# ]\n",
        "\n",
        "# # Preprocessing the corpus\n",
        "# tokenized_corpus = [document.lower().split() for document in corpus]\n",
        "\n",
        "# # Creating the dictionary\n",
        "# dictionary = corpora.Dictionary(tokenized_corpus)\n",
        "\n",
        "# # Creating the document-term matrix\n",
        "# doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in tokenized_corpus]\n",
        "\n",
        "# # Training the LDA model\n",
        "# lda_model = models.LdaModel(\n",
        "#     corpus=doc_term_matrix,\n",
        "#     id2word=dictionary,\n",
        "#     num_topics=3,\n",
        "#     passes=10\n",
        "# )\n",
        "\n",
        "# Prepare the data for visualization\n",
        "lda_vis_data = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n",
        "\n",
        "# Display the interactive topic visualization\n",
        "pyLDAvis.display(lda_vis_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApGfuqklwDs_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "import pyLDAvis\n",
        "\n",
        "# # Example corpus of documents\n",
        "# corpus = [\n",
        "#     \"This is the first document\",\n",
        "#     \"This document is the second document\",\n",
        "#     \"And this is the third one\",\n",
        "#     \"Is this the first document?\"\n",
        "# ]\n",
        "\n",
        "# Initialize and train the Bertopic model\n",
        "bertopic_model = BERTopic()\n",
        "topics, _ = bertopic_model.fit_transform(docs)\n",
        "\n",
        "# Convert the topics to a Pandas DataFrame\n",
        "df_topics = pd.DataFrame(topics, columns=[\"Topic\"])\n",
        "\n",
        "# Generate the topic visualization using pyLDAvis\n",
        "lda_vis_data = pyLDAvis.prepare(\n",
        "    df_topics,\n",
        "    bertopic_model.transform(docs),\n",
        "    bertopic_model.get_topics(),\n",
        "    df_topics[\"Topic\"].value_counts(),\n",
        "    R=10,  # Number of relevant terms to display per topic\n",
        "    lambda_step=0.01\n",
        ")\n",
        "\n",
        "# Display the interactive topic visualization\n",
        "pyLDAvis.display(lda_vis_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkR5yJXswDql"
      },
      "outputs": [],
      "source": [
        "%pip show pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvsR34hmwDoT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0yBckH8wDl5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-wbJfySTl6fU",
        "jWlLeyIPl9Qp",
        "aSEQn2lKe_Ub",
        "jaP_tMYU-gE0",
        "eWiz7u1_3Y2x",
        "eN9R03nzkIG2"
      ],
      "authorship_tag": "ABX9TyMzlvwBNZQXn2GONZFVrW0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}